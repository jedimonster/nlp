{
 "metadata": {
  "signature": "sha256:867c402bc289036700c3c4baed043e54e8632b819cdc83aaf71903b56c8a1989"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Show that this version of Naive Bayes implements a version of the \"all-vs-all\" multi-class strategy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the linked article seems to be inventing greek letters on the go, we used <a href=\"https://www.ke.tu-darmstadt.de/~juffi/publications/ecml-07-NaiveBayes.pdf\">this</a> paper for inspiration and reference.\n",
      "Looking at NaiveBayesClassifier.prob_classify(feature_set):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "for label in self._labels:\n",
      "    logprob[label] = self._label_probdist.logprob(label)\n",
      "\n",
      "    # Then add in the log probability of features given labels.\n",
      "for label in self._labels:\n",
      "    for (fname, fval) in featureset.items():\n",
      "        if (label, fname) in self._feature_probdist:\n",
      "            feature_probs = self._feature_probdist[label, fname]\n",
      "            logprob[label] += feature_probs.logprob(fval)\n",
      "        else:\n",
      "            # nb: This case will never come up if the\n",
      "            # classifier was created by\n",
      "            # NaiveBayesClassifier.train().\n",
      "            logprob[label] += sum_logs([])  # = -INF.\n",
      "</pre>                    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Those lines are the python equivalent of the log of the following estimation for C, where the argmax is performed in the calling line <br/><pre>\n",
      "self.prob_classify(featureset).max()\n",
      "</pre>\n",
      "<img src=\"./pictures/naive-baise-likelihood.png\" /><br/>\n",
      "which means, nltk implements the 'normal' naive bayse classifier, but according to the paper linked, it is equivalent to the pairwise (one vs one) version (conclusion in abstract, proof in section 4."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running the original nltk naive baise"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q3.3 One-vs-all Naive Bayes for Multi-label Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we note the nltk implementation of the naive bayse classifier expects discrete feature values, so we use binary bag of words.\n",
      "Let's create a bag of words for the K most frequent words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import reuters\n",
      "from collections import defaultdict\n",
      "\n",
      "words = reuters.words()\n",
      "word_freq = defaultdict(int)\n",
      "for word in words:\n",
      "    word_freq[word] += 1\n",
      "\n",
      "sorted_words = sorted(set(words), key=word_freq.get, reverse=True)\n",
      "\n",
      "for word in sorted_words[:10]:\n",
      "    print word, \":\", word_freq[word]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". : 94687\n",
        ", : 72360\n",
        "the : 58251\n",
        "of : 35979\n",
        "to : 34035\n",
        "in : 26478\n",
        "said : 25224\n",
        "and : 25043\n",
        "a : 23492\n",
        "mln : 18037\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we notice that we get a lot of punctuation marks, so we'll make the assumption one letter words are useless:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_frequent_words = filter(lambda w: len(w) > 1, sorted_words)\n",
      "print len(most_frequent_words)\n",
      "print most_frequent_words[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41524\n",
        "[u'the', u'of', u'to', u'in', u'said', u'and', u'mln', u'vs', u'for', u'dlrs']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That makes more sense, however, it'll probably be wrong to take the most frequent words because they don't give us a lot of information - the will appear in most documents regardless of category.<br/>\n",
      "Nevertheless, a feature will get the value 1 iff the word is in the document, and the featureset will be a dict of word->bool, in accordance with nltk. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words(document, words):\n",
      "    document_set = set(document)\n",
      "    intersection = document_set.intersection(words)\n",
      "    return dict([(word, (word in intersection)) for word in words])\n",
      "\n",
      "def bag_of_words_freq(document, words):\n",
      "    bag = dict((word, 0) for word in words)\n",
      "    \n",
      "    for word in document:\n",
      "        if word in words:\n",
      "            bag[word] += 1\n",
      "            \n",
      "    return bag\n",
      "        \n",
      "def get_dataset(K, feature_extractor):\n",
      "    train_featuresets = list()  # list of pairs (featureset, category)\n",
      "    test_featuresets = list()\n",
      "\n",
      "    for category in reuters.categories():\n",
      "        for fileid in reuters.fileids(categories=category):\n",
      "            featureset = feature_extractor(reuters.words(fileids=[fileid]), most_frequent_words[:K])\n",
      "            if fileid[:4] == 'test':\n",
      "                test_featuresets.append( (featureset, category) )\n",
      "            else:\n",
      "                train_featuresets.append((featureset, category))\n",
      "\n",
      "    return train_featuresets, test_featuresets\n",
      "\n",
      "train_featuresets, test_featuresets = get_dataset(1000, bag_of_words)\n",
      "print len(train_featuresets)\n",
      "print len(test_featuresets)\n",
      "# print train_featuresets[0][:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9584\n",
        "3744\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
      "import time\n",
      "from joblib import Parallel, delayed  # For classification is a long task, and the deadline is near.\n",
      "import multiprocessing\n",
      "\n",
      "num_cores = multiprocessing.cpu_count()\n",
      "start = time.time()\n",
      "\n",
      "classifer = NaiveBayesClassifier.train(train_featuresets)\n",
      "print \"finished training, now Calculating on %d cores\" % num_cores\n",
      "\n",
      "def classify(item):\n",
      "    featureset, tag = item\n",
      "    return 1 if classifer.classify(featureset) == tag else 0\n",
      "\n",
      "binary_list = Parallel(n_jobs=num_cores,  max_nbytes=1e3)(delayed(classify)(item) for item in test_featuresets)\n",
      "#     \n",
      "correct = sum(binary_list)\n",
      "# correct = 0\n",
      "# for featureset, tag in test_featuresets:\n",
      "#     if tag == classifer.classify(featureset):\n",
      "#         correct += 1\n",
      "\n",
      "print float(correct) / len(test_featuresets)\n",
      "\n",
      "end = time.time()\n",
      "print \"finished in %fs\" % (end-start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finished training, now Calculating on 4 cores\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.580662393162\n",
        "finished in 264.168278s\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Binary bag of words for K=1000: 0.580662393162<br/>\n",
      "Binary bag of words for K=2000: 0.600427350427<br/>\n",
      "<br/>\n",
      "Frequency bag of words yields 0.5625 for K=1000, which confirms the earlier claim that the NaiveBayesClassifier in nltk does not support numerical features. <br/>\n",
      "Also tested the hypothesis about punctuation marks, if we don't remove them we get (with K=1000:) 0.575587606838<br/>\n",
      "<br/>\n",
      "Now, K=2000 gives marginally better results, but we can't parallelize it using joblib because the O/S runs out of memory (joblib forks the python process), so we'll stick to K=1000 and revisit this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}