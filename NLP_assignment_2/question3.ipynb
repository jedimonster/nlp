{
 "metadata": {
  "signature": "sha256:eea33a50d86ea4232a4031d74242ca6d08077c8e5e2f428eac4403cd9af88085"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Show that this version of Naive Bayes implements a version of the \"all-vs-all\" multi-class strategy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the linked article seems to be inventing greek letters on the go, we used <a href=\"https://www.ke.tu-darmstadt.de/~juffi/publications/ecml-07-NaiveBayes.pdf\">this</a> paper for inspiration and reference.\n",
      "Looking at NaiveBayesClassifier.prob_classify(feature_set):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "for label in self._labels:\n",
      "    logprob[label] = self._label_probdist.logprob(label)\n",
      "\n",
      "    # Then add in the log probability of features given labels.\n",
      "for label in self._labels:\n",
      "    for (fname, fval) in featureset.items():\n",
      "        if (label, fname) in self._feature_probdist:\n",
      "            feature_probs = self._feature_probdist[label, fname]\n",
      "            logprob[label] += feature_probs.logprob(fval)\n",
      "        else:\n",
      "            # nb: This case will never come up if the\n",
      "            # classifier was created by\n",
      "            # NaiveBayesClassifier.train().\n",
      "            logprob[label] += sum_logs([])  # = -INF.\n",
      "</pre>                    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Those lines are the python equivalent of the log of the following estimation for C, where the argmax is performed in the calling line <br/><pre>\n",
      "self.prob_classify(featureset).max()\n",
      "</pre>\n",
      "<img src=\"./pictures/naive-baise-likelihood.png\" /><br/>\n",
      "which means, nltk implements the 'normal' naive bayse classifier, but according to the paper linked, it is equivalent to the pairwise (one vs one) version (conclusion in abstract, proof in section 4."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running the original nltk naive baise"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q3.3 One-vs-all Naive Bayes for Multi-label Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we note the nltk implementation of the naive bayse classifier expects discrete feature values, so we use binary bag of words.\n",
      "Let's create a bag of words for the K most frequent words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import reuters\n",
      "from collections import defaultdict\n",
      "\n",
      "words = reuters.words()\n",
      "word_freq = defaultdict(int)\n",
      "for word in words:\n",
      "    word_freq[word] += 1\n",
      "\n",
      "sorted_words = sorted(set(words), key=word_freq.get, reverse=True)\n",
      "\n",
      "for word in sorted_words[:10]:\n",
      "    print word, \":\", word_freq[word]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". : 94687\n",
        ", : 72360\n",
        "the : 58251\n",
        "of : 35979\n",
        "to : 34035\n",
        "in : 26478\n",
        "said : 25224\n",
        "and : 25043\n",
        "a : 23492\n",
        "mln : 18037\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we notice that we get a lot of punctuation marks, so we'll make the assumption one letter words are useless:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_frequent_words = filter(lambda w: len(w) > 1, sorted_words)\n",
      "print len(most_frequent_words)\n",
      "print most_frequent_words[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41524\n",
        "[u'the', u'of', u'to', u'in', u'said', u'and', u'mln', u'vs', u'for', u'dlrs']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That makes more sense, however, it'll probably be wrong to take the most frequent words because they don't give us a lot of information - the will appear in most documents regardless of category.<br/>\n",
      "Nevertheless, a feature will get the value 1 iff the word is in the document, and the featureset will be a dict of word->bool, in accordance with nltk. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words(document, words):\n",
      "    document_set = set(document)\n",
      "    intersection = document_set.intersection(words)\n",
      "    return dict([(word, (word in intersection)) for word in words])\n",
      "\n",
      "def bag_of_words_freq(document, words):\n",
      "    bag = dict((word, 0) for word in words)\n",
      "    \n",
      "    for word in document:\n",
      "        if word in words:\n",
      "            bag[word] += 1\n",
      "            \n",
      "    return bag\n",
      "        \n",
      "def get_dataset(K, feature_extractor):\n",
      "    train_featuresets = list()  # list of pairs (featureset, category)\n",
      "    test_featuresets = list()\n",
      "\n",
      "    for category in reuters.categories():\n",
      "        for fileid in reuters.fileids(categories=category):\n",
      "            featureset = feature_extractor(reuters.words(fileids=[fileid]), most_frequent_words[:K])\n",
      "            if fileid[:4] == 'test':\n",
      "                test_featuresets.append( (featureset, category) )\n",
      "            else:\n",
      "                train_featuresets.append((featureset, category))\n",
      "\n",
      "    return train_featuresets, test_featuresets\n",
      "\n",
      "train_featuresets, test_featuresets = get_dataset(1000, bag_of_words)\n",
      "print len(train_featuresets)\n",
      "print len(test_featuresets)\n",
      "# print train_featuresets[0][:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9584\n",
        "3744\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
      "import time\n",
      "from joblib import Parallel, delayed  # For classification is a long task, and the deadline is near.\n",
      "import multiprocessing\n",
      "import sys\n",
      "    \n",
      "num_cores = multiprocessing.cpu_count()\n",
      "start = time.time()\n",
      "\n",
      "classifer = NaiveBayesClassifier.train(train_featuresets)\n",
      "parallel = True\n",
      "\n",
      "def classify(item):\n",
      "        featureset, tag = item\n",
      "        return 1 if classifer.classify(featureset) == tag else 0\n",
      "\n",
      "if parallel:\n",
      "    print \"Finished training, now classifying on %d cores\" % num_cores    \n",
      "    sys.stdout.flush()\n",
      "    \n",
      "    binary_list = Parallel(n_jobs=num_cores)(delayed(classify)(item) for item in test_featuresets)\n",
      "    #     \n",
      "    correct = sum(binary_list)\n",
      "else:\n",
      "    print \"Finished training, classifying on 1 core\"\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "    correct = 0\n",
      "    for featureset, tag in test_featuresets:\n",
      "        if tag == classifer.classify(featureset):\n",
      "            correct += 1\n",
      "\n",
      "print float(correct) / len(test_featuresets)\n",
      "\n",
      "end = time.time()\n",
      "print \"finished in %fs\" % (end-start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished training, now classifying on 8 cores\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.580662393162\n",
        "finished in 244.481296s\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Binary bag of words for K=1000: 0.580662393162<br/>\n",
      "Binary bag of words for K=2000: 0.600427350427<br/>\n",
      "Freqeuncy bag of words for K=1000: 0.5625<br/>\n",
      "Freqeuncy bag of words for K=2000: 0.571581196581<br/>\n",
      "<br/>\n",
      "Frequency bag of words yields 0.5625 for K=1000, which confirms the earlier claim that the NaiveBayesClassifier in nltk does not support numerical features. <br/>\n",
      "Also tested the hypothesis about punctuation marks, if we don't remove them we get (with K=1000:) 0.575587606838<br/>\n",
      "<br/>\n",
      "Now, K=2000 gives marginally better results, but we can't parallelize it using joblib because the O/S runs out of memory (joblib forks the python process), so we'll stick to K=1000 and revisit this."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we gave no proof or reason why the nltk naive bayes classifier does not support numeric features.<br/>\n",
      "We will now train a classifier with two classes and one feature which will have 2 numeric values that should make it linearly separateable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
      "\n",
      "a_featureset = ({'feature': 0}, 'a')\n",
      "b_featureset = ({'feature': 9}, 'b')\n",
      "\n",
      "classifier = NaiveBayesClassifier.train((a_featureset, b_featureset))\n",
      "\n",
      "# now look at the smoothed probabilities for feature values:\n",
      "a_prob = classifier._feature_probdist['a','feature']\n",
      "b_prob = classifier._feature_probdist['b','feature']\n",
      "\n",
      "# we expect feature values near 0/10 will have higher probability in a/b accordiangly. but they don't:\n",
      "print [ a_prob.prob(i)for i in range(10) ]\n",
      "print [ b_prob.prob(i)for i in range(10) ]\n",
      "print [classifier.classify({'feature': i}) for i in range(10)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that as initially claimed, values that weren't seen during training have a uniform distribution.<br/>\n",
      "That wasn't fun.<br/>\n",
      "We'll have to replace ELEProbDist with some Gaussian model, but there isn't any in nltk, so we'll use sklearn's one.<br/>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import SklearnClassifier\n",
      "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
      "\n",
      "classifier = SklearnClassifier(GaussianNB(), sparse=False)\n",
      "classifier.train((a_featureset, b_featureset))\n",
      "\n",
      "print [classifier.classify({'feature': i}) for i in range(10)]\n",
      "print classifier.prob_classify_many({'feature': i } for i in range(10))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
        "[<ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>, <ProbDist with 2 samples>]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "now let us try frequency bag of words again.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from joblib import Parallel, delayed  # For classification is a long task, and the deadline is near.\n",
      "import multiprocessing\n",
      "num_cores = multiprocessing.cpu_count()\n",
      "start = time.time()\n",
      "parallel = False\n",
      "\n",
      "classifier = SklearnClassifier(GaussianNB(), sparse=False)\n",
      "classifier.train(train_featuresets)\n",
      "\n",
      "\n",
      "def classify(item):\n",
      "        featureset, tag = item\n",
      "        return 1 if classifier.classify(featureset) == tag else 0\n",
      "\n",
      "if parallel:\n",
      "    print \"Finished training, now classifying on %d cores\" % num_cores\n",
      "    binary_list = []\n",
      "    binary_list = Parallel(n_jobs=num_cores,  max_nbytes=1e3)(delayed(classify)(item) for item in test_featuresets)\n",
      "    #     \n",
      "    correct = sum(binary_list)\n",
      "else:\n",
      "    print \"Finished training, classifying on 1 core\"\n",
      "    correct = 0\n",
      "    for featureset, tag in test_featuresets:\n",
      "        if tag == classifier.classify(featureset):\n",
      "            correct += 1\n",
      "            \n",
      "print float(correct) / len(test_featuresets)\n",
      "\n",
      "\n",
      "end = time.time()\n",
      "print \"finished in %fs\" % (end-start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished training, classifying on 1 core\n",
        "0.5625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "finished in 65.433417s\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
      "from nltk import SklearnClassifier\n",
      "\n",
      "classifier = SklearnClassifier(OneVsRestClassifier(MultinomialNB(alpha=0.5)), sparse=False)\n",
      "classifier.train(train_featuresets)\n",
      "\n",
      "correct = 0\n",
      "for featureset, tag in test_featuresets:\n",
      "    if tag == classifier.classify(featureset):\n",
      "        correct += 1\n",
      "            \n",
      "print float(correct) / len(test_featuresets)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.662927350427\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Binary bag of words:</b><br/>\n",
      "0.6813 for multinomial<br/>\n",
      "0.5587 for gaussian<br/>\n",
      "<br/>\n",
      "0.537 for binarized gaussian<br/>\n",
      "0.68 for binarized multinomial<br/>\n",
      "\n",
      "<b>Freq bag of words:</b><br/>\n",
      "0.6634 binarized Multinomial (0.6629 w/ alpha = 0.5)<br/>\n",
      "0.6837 Multinomial (0.6813 w/ alpha = 0.5)<br/> \n",
      "0.5395 Binarized Gaussian<br/>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}