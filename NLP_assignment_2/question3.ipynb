{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Show that this version of Naive Bayes implements a version of the \"all-vs-all\" multi-class strategy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the linked article seems to be inventing greek letters on the go, we used <a href=\"https://www.ke.tu-darmstadt.de/~juffi/publications/ecml-07-NaiveBayes.pdf\">this</a> paper for inspiration and reference.\n",
      "Looking at NaiveBayesClassifier.prob_classify(feature_set):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "for label in self._labels:\n",
      "    logprob[label] = self._label_probdist.logprob(label)\n",
      "\n",
      "    # Then add in the log probability of features given labels.\n",
      "for label in self._labels:\n",
      "    for (fname, fval) in featureset.items():\n",
      "        if (label, fname) in self._feature_probdist:\n",
      "            feature_probs = self._feature_probdist[label, fname]\n",
      "            logprob[label] += feature_probs.logprob(fval)\n",
      "        else:\n",
      "            # nb: This case will never come up if the\n",
      "            # classifier was created by\n",
      "            # NaiveBayesClassifier.train().\n",
      "            logprob[label] += sum_logs([])  # = -INF.\n",
      "</pre>                    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Those lines are the python equivalent of the log of the following estimation for C, where the argmax is performed in the calling line <br/><pre>\n",
      "self.prob_classify(featureset).max()\n",
      "</pre>\n",
      "<img src=\"./pictures/naive-baise-likelihood.png\" /><br/>\n",
      "which means, nltk implements the 'normal' naive bayse classifier, but according to the paper linked, it is equivalent to the pairwise (one vs one) version (conclusion in abstract, proof in section 4."
     ]
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Running the original nltk naive baise"
     ]
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Q3.3 One-vs-all Naive Bayes for Multi-label Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we note the nltk implementation of the naive bayse classifier expects discrete feature values, so we use binary bag of words.\n",
      "Let's create a bag of words for the K most frequent words."
     ]
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": ". : 94687\n, : 72360\nthe : 58251\nof : 35979\nto : 34035\nin : 26478\nsaid : 25224\nand : 25043\na : 23492\nmln : 18037\n",
       "text": [
        ". : 94687\n, : 72360\nthe : 58251\nof : 35979\nto : 34035\nin : 26478\nsaid : 25224\nand : 25043\na : 23492\nmln : 18037\n"
       ]
      }
     ],
     "input": [
      "from nltk.corpus import reuters\n",
      "from collections import defaultdict\n",
      "\n",
      "words = reuters.words()\n",
      "word_freq = defaultdict(int)\n",
      "for word in words:\n",
      "    word_freq[word] += 1\n",
      "\n",
      "sorted_words = sorted(set(words), key=word_freq.get, reverse=True)\n",
      "\n",
      "for word in sorted_words[:10]:\n",
      "    print word, \":\", word_freq[word]"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we notice that we get a lot of punctuation marks, so we'll make the assumption one letter words are useless:"
     ]
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "50\n[u'the', u'of', u'to', u'in', u'said', u'and', u'mln', u'vs', u'for', u'dlrs']\n",
       "text": [
        "50\n[u'the', u'of', u'to', u'in', u'said', u'and', u'mln', u'vs', u'for', u'dlrs']\n"
       ]
      }
     ],
     "input": [
      "most_frequent_words = filter(lambda w: len(w) > 1, sorted_words)[:K]\n",
      "print len(most_frequent_words)\n",
      "print most_frequent_words[:10]"
     ],
     "language": "python",
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That makes more sense, however, it'll probably be wrong to take the most frequent words because they don't give us a lot of information - the will appear in most documents regardless of category.<br/>\n",
      "Nevertheless, a feature will get the value 1 iff the word is in the document, and the featureset will be a dict of word->bool, in accordance with nltk. "
     ]
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "9584\n3744\n({u'and': True, u'is': False, u'year': False, u'share': True, u'it': True, u'an': True, u'as': False, u'vs': False, u'at': True, u'have': False, u'in': False, u'Net': False, u'its': True, u'1986': False, u'said': True, u'from': False, u'for': True, u'had': False, u'pct': True, u'to': True, u'lt': True, u'which': False, u'has': True, u'was': False, u'be': True, u'Shr': False, u'that': False, u'company': True, u'000': True, u'loss': False, u'not': True, u',\"': False, u'The': True, u'on': False, u'with': False, u'by': False, u'he': False, u'billion': False, u'dlrs': True, u'last': False, u'would': True, u'cts': False, u'this': False, u'of': True, u'mln': True, u'will': False, u'were': True, u'the': True, u'or': False, u'are': True}, u'acq')\n",
       "text": [
        "9584\n3744\n({u'and': True, u'is': False, u'year': False, u'share': True, u'it': True, u'an': True, u'as': False, u'vs': False, u'at': True, u'have': False, u'in': False, u'Net': False, u'its': True, u'1986': False, u'said': True, u'from': False, u'for': True, u'had': False, u'pct': True, u'to': True, u'lt': True, u'which': False, u'has': True, u'was': False, u'be': True, u'Shr': False, u'that': False, u'company': True, u'000': True, u'loss': False, u'not': True, u',\"': False, u'The': True, u'on': False, u'with': False, u'by': False, u'he': False, u'billion': False, u'dlrs': True, u'last': False, u'would': True, u'cts': False, u'this': False, u'of': True, u'mln': True, u'will': False, u'were': True, u'the': True, u'or': False, u'are': True}, u'acq')\n"
       ]
      }
     ],
     "input": [
      "def bag_of_words(document, words):\n",
      "    document_set = set(document)\n",
      "    intersection = document_set.intersection(words)\n",
      "    return dict([(word, (word in intersection)) for word in words])\n",
      "\n",
      "\n",
      "def get_dataset(K):\n",
      "    train_featuresets = list()  # list of pairs (featureset, category)\n",
      "    test_featuresets = list()\n",
      "\n",
      "    for category in reuters.categories():\n",
      "        for fileid in reuters.fileids(categories=[category]):\n",
      "            featureset = bag_of_words(reuters.words(fileids=[fileid]), most_frequent_words[:K])\n",
      "            if fileid[:4] == 'test':\n",
      "                test_featuresets.append( (featureset, category) )\n",
      "            else:\n",
      "                train_featuresets.append((featureset, category))\n",
      "\n",
      "    return train_featuresets, test_featuresets\n",
      "\n",
      "train_featuresets, test_featuresets = get_dataset(1000)\n",
      "print len(train_featuresets)\n",
      "print len(test_featuresets)\n",
      "print train_featuresets[0]"
     ],
     "language": "python",
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [],
     "input": [
      "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
      "\n",
      "classifer = NaiveBayesClassifier.train(train_featuresets)\n",
      "\n",
      "correct = 0\n",
      "print classifer.classify(test_featuresets[0][0])\n",
      "# for featureset, tag in train_featuresets:\n",
      "#     if tag == classifer.classify(featureset):\n",
      "#         correct += 1\n",
      "#         \n",
      "# print correct / len(test_featuresets)"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [],
     "input": [
      ""
     ],
     "language": "python"
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {
  "signature": "sha256:a530a5265c18bece1dfc8a7205710a24b23ac1019fb0164690c5894234cfa7a0"
 },
 "nbformat": 3,
 "nbformat_minor": 0
}