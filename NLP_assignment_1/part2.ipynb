{
 "metadata": {
  "signature": "sha256:c436e67697b60a10e9639e8c869843f49849e6afe45a15bbd14e356a1de4225e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.part2 import *\n",
      "from nltk import UnigramTagger\n",
      "\n",
      "all_words = corpus.brown.tagged_sents(tagset='universal')\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "\n",
      "our_tagger = SimpleUnigramTagger(train=train)\n",
      "nltk_tagger = UnigramTagger(train=train)\n",
      "\n",
      "print our_tagger.evaluate(test)\n",
      "print nltk_tagger.evaluate(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the above code takes a long time, results:\n",
      "building our tagger took  0:00:13.400085\n",
      "building nltk tagger took  0:00:20.180736\n",
      "0.908655550217\n",
      "0.908655550217"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Filtering Entropy Affix Tagger"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does entropy filtering improve accuracy?\n",
      "that depends on the way we measure accuracy. if we consider None as a mistake when calculating accuracy rate, using Entropy gives no adventage, because avoiding the mistakes caused by high entropy resulted in more 'None's, which are consdiered as bad.\n",
      "However, in a real environement where the tagger is part of a backoff or a voting strategy, entropy can be useful.\n",
      "For example FilteringExtripyAffixTageer with UnigramTagger as backoff will yield higher accuracy that AffixTagger with UnigramTagger as backoff tagger."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = corpus.brown.tagged_sents(tagset='universal')\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "from nltk.tag import UnigramTagger\n",
      "cutoffs = range(0, 15)\n",
      "cutoffs = [x*0.1 for x in cutoffs]\n",
      "u0 = UnigramTagger(train)\n",
      "nt = AffixTagger(train=train, backoff=u0)\n",
      "\n",
      "print \"result of evaluating nltk affix tagger: \", nt.evaluate(test)\n",
      "print \"finding best cutoff\"\n",
      "for cutoff in cutoffs:\n",
      "    print \"cutoff: \", cutoff\n",
      "    eat = EntropyAffixTagger(train=train, cutoff=cutoff, backoff=u0)\n",
      "    print \"evaluating entropy affix tagger: \", eat.evaluate(dev)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We had following results:\n",
      "result of evaluating nltk affix tagger:  0.87488874361\n",
      "finding best cutoff\n",
      "cutoff:  0.0\n",
      "evaluating entropy affix tagger:  0.905657166474\n",
      "cutoff:  0.1\n",
      "evaluating entropy affix tagger:  0.909708997496\n",
      "cutoff:  0.2\n",
      "evaluating entropy affix tagger:  0.914020145704\n",
      "cutoff:  0.3\n",
      "evaluating entropy affix tagger:  0.916378311359\n",
      "cutoff:  0.4\n",
      "evaluating entropy affix tagger:  0.91796662912\n",
      "cutoff:  0.5\n",
      "evaluating entropy affix tagger:  0.918590611097\n",
      "cutoff:  0.6\n",
      "evaluating entropy affix tagger:  0.917585757004\n",
      "cutoff:  0.7\n",
      "evaluating entropy affix tagger:  0.916021750229\n",
      "cutoff:  0.8\n",
      "evaluating entropy affix tagger:  0.913614962602\n",
      "cutoff:  0.9\n",
      "evaluating entropy affix tagger:  0.907845155226\n",
      "cutoff:  1.0\n",
      "evaluating entropy affix tagger:  0.902853299406\n",
      "cutoff:  1.1\n",
      "evaluating entropy affix tagger:  0.899563212616\n",
      "cutoff:  1.2\n",
      "evaluating entropy affix tagger:  0.895876046385\n",
      "cutoff:  1.3\n",
      "evaluating entropy affix tagger:  0.89014675732\n",
      "cutoff:  1.4\n",
      "evaluating entropy affix tagger:  0.888979829985\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we can see that the higher the cutoff the closer our results are to the results of the normal AffixTagger which doesn't filter out anything. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.part2 import *\n",
      "from nltk import UnigramTagger\n",
      "\n",
      "all_words = corpus.brown.tagged_sents(tagset='universal')\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "\n",
      "u0 = UnigramTagger(train)\n",
      "\n",
      "eat = EntropyAffixTagger(train=train, cutoff=0.5, backoff=u0)\n",
      "print \"evaluating entropy affix tagger: \", eat.evaluate(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "evaluating entropy affix tagger:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.922408021487\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result we get is:\n",
      "    evaluating entropy affix tagger:  0.922408021487\n",
      "When we run Entropy tagger with best cutoff on test, we get much better results than Affix tagger"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "2. how do you determine the range of values to test for the cutoff?\n",
      "Empirically by looking at possible values for the entroy. we know 0 is the minimum, and we saw the max is around 1.5.\n",
      "3. is the accuracy value evolving in a predictable manner as the cutoff varies?\n",
      "Yes - as seen above. \n",
      "4. describe the list of suffixes that are good tag predictors -- are you surprised by what you observe?\n",
      "in the list we observed both reasonable suffixes (like 'gly', 'sly' and the rest of the 'ly's) and semingly random ones (like 'wda'). This is what we exepcted - that's what happens in the normal affix tagger."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Entropy Voting"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "1.\n",
      "The entropy voting tagger scored:\n",
      "    0.882371474255\n",
      "while running Unigram with Affix backoff scored:\n",
      "    0.938397435393\n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "we can see the entropy backoff strategy is not necessarily better. we imagine the heap of statistics we're about to generate will help us understand why.\n",
      "2.1. UnigramTagger (with no backoff) has no information (unknown word)\n",
      "tokens: 6921 (5.6%)\n",
      "unique words: 4757 (29%)\n",
      "\n",
      "2.2. UnigramTagger has exactly i tag options (for i in [1...12]).\n",
      "tokens: \n",
      "{1: 56131, 2: 37627, 3: 13254, 4: 8261, 5: 1144, 6: 69}\n",
      "{1: 0.45, 2: 0.3, 3: 0.11, 4: 0.07, 5: 0.01, 6: 0.0}\n",
      "\n",
      "unique words: \n",
      "{1: 9260, 2: 1750, 3: 228, 4: 43, 5: 6, 6: 1}\n",
      "{1: 0.58, 2: 0.11, 3: 0.01, 4: 0.0, 5: 0.0, 6: 0.0}\n",
      "\n",
      "2.3. UnigramTagger has an entropy over 0.69 (which is the entropy of a 50%/50% bet).\n",
      "tokens: 11687 (9.47%)\n",
      "\n",
      "3. Draw a plot of the error rate of the unigram tagger as a function of the entropy.\n",
      "Assuming we consider None an error:\n",
      "error rates (None=error) =  [(0.0, 0.5770996936018715), (0.1, 0.27396246032183613), (0.2, 0.22664797296765093), (0.3, 0.21289550169739835), (0.4, 0.19455099677848753), (0.5, 0.17860884222465523), (0.6, 0.16773131483392278), (0.7, 0.1534274844634882), (0.8, 0.1427941303885506), (0.9, 0.13102655187895307), (1.0, 0.10392331382572328), (1.1, 0.1004024921431328), (1.2, 0.09960695972715605), (1.3, 0.09878779763545709), (1.4, 0.09232114304618022)]\n",
      "\n",
      "4. First plot is where None concidered as errors.\n",
      "As we can see, when entropy grows ,the error error dicreasing.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"graphs/error_rate_unigram_nones_are_error.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this graph Nones are not concidered as errors:\n",
      "<img src=\"graphs/error_rate_unigram_nones_are_correct.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see, when entropy grows , we have more mistakes( not None mistakes), that we cannot fix in backoff tagger."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "looking at the disagreements, they were all when the voting tagger returned None.\n",
      "The conclusion is in the voting method, the Affix tagger was never preferred over the Unigram tagger. this makes sense - whenever the unigram tagger had a few options for a word, the Affix tagger had <b>more</b> options, since it was looking at the affix of the one. as a result, it had highe entropy as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}