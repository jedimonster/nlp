{
 "metadata": {
  "signature": "sha256:93464f03fdae7f9f5bb6f48f33a763a937dbed2d1fc665ccd9721eb657ec40a5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.part2 import *\n",
      "from nltk import UnigramTagger\n",
      "\n",
      "all_words = corpus.brown.tagged_sents(tagset='universal')\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "\n",
      "our_tagger = SimpleUnigramTagger(train=train)\n",
      "nltk_tagger = UnigramTagger(train=train)\n",
      "\n",
      "print our_tagger.evaluate(test), \"\\n\"\n",
      "print nltk_tagger.evaluate(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.908655550217 \n",
        "\n",
        "0.908655550217"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "the above code takes a long time, results:\n",
      "building our tagger took  0:00:13.400085\n",
      "building nltk tagger took  0:00:20.180736\n",
      "0.908655550217\n",
      "0.908655550217"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Filtering Entropy Affix Tagger"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does entropy filtering improve accuracy?\n",
      "that depends on the way we measure accuracy. if we consider None as a mistake when calculating accuracy rate, using Entropy gives no adventage, because avoiding the mistakes caused by high entropy resulted in more 'None's, which are consdiered as bad.\n",
      "However, in a real environement where the tagger is part of a backoff or a voting strategy, entropy can be useful.\n",
      "For example FilteringExtripyAffixTageer with UnigramTagger as backoff will yield higher accuracy that AffixTagger with UnigramTagger as backoff tagger."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = corpus.brown.tagged_sents(tagset='universal')\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "from nltk.tag import UnigramTagger\n",
      "cutoffs = range(0, 15)\n",
      "cutoffs = [x*0.1 for x in cutoffs]\n",
      "u0 = UnigramTagger(train)\n",
      "nt = AffixTagger(train=train, backoff=u0)\n",
      "\n",
      "print \"result of evaluating nltk affix tagger: \", nt.evaluate(test)\n",
      "print \"finding best cutoff\"\n",
      "for cutoff in cutoffs:\n",
      "    print \"cutoff: \", cutoff\n",
      "    eat = EntropyAffixTagger(train=train, cutoff=cutoff, backoff=u0)\n",
      "    print \"evaluating entropy affix tagger: \", eat.evaluate(dev)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We had following results:\n",
      "result of evaluating nltk affix tagger:  0.87488874361\n",
      "finding best cutoff\n",
      "cutoff:  0.0\n",
      "evaluating entropy affix tagger:  0.905657166474\n",
      "cutoff:  0.1\n",
      "evaluating entropy affix tagger:  0.909708997496\n",
      "cutoff:  0.2\n",
      "evaluating entropy affix tagger:  0.914020145704\n",
      "cutoff:  0.3\n",
      "evaluating entropy affix tagger:  0.916378311359\n",
      "cutoff:  0.4\n",
      "evaluating entropy affix tagger:  0.91796662912\n",
      "cutoff:  0.5\n",
      "evaluating entropy affix tagger:  0.918590611097\n",
      "cutoff:  0.6\n",
      "evaluating entropy affix tagger:  0.917585757004\n",
      "cutoff:  0.7\n",
      "evaluating entropy affix tagger:  0.916021750229\n",
      "cutoff:  0.8\n",
      "evaluating entropy affix tagger:  0.913614962602\n",
      "cutoff:  0.9\n",
      "evaluating entropy affix tagger:  0.907845155226\n",
      "cutoff:  1.0\n",
      "evaluating entropy affix tagger:  0.902853299406\n",
      "cutoff:  1.1\n",
      "evaluating entropy affix tagger:  0.899563212616\n",
      "cutoff:  1.2\n",
      "evaluating entropy affix tagger:  0.895876046385\n",
      "cutoff:  1.3\n",
      "evaluating entropy affix tagger:  0.89014675732\n",
      "cutoff:  1.4\n",
      "evaluating entropy affix tagger:  0.888979829985\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we can see that the higher the cutoff the closer our results are to the results of the normal AffixTagger which doesn't filter out anything. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we now try the affix tagger with a backoff over the test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.part2 import *\n",
      "from nltk import UnigramTagger\n",
      "\n",
      "all_words = corpus.brown.tagged_sents(tagset='universal')\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "\n",
      "u0 = UnigramTagger(train)\n",
      "\n",
      "eat = EntropyAffixTagger(train=train, cutoff=0.5, backoff=u0)\n",
      "print \"evaluating entropy affix tagger: \", eat.evaluate(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "evaluating entropy affix tagger:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.922408021487\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result we get is:\n",
      "    evaluating entropy affix tagger:  0.922408021487\n",
      "When we run Entropy tagger with best cutoff on test, we get much better results than Affix tagger with Unigram backoff."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "2. how do you determine the range of values to test for the cutoff?\n",
      "Empirically by looking at possible values for the entroy. we know 0 is the minimum, and we saw the max is around 1.5.\n",
      "3. is the accuracy value evolving in a predictable manner as the cutoff varies?\n",
      "Yes - as seen above. \n",
      "4. describe the list of suffixes that are good tag predictors -- are you surprised by what you observe?\n",
      "in the list we observed both reasonable suffixes (like 'gly', 'sly' and the rest of the 'ly's) and semingly random ones (like 'wda'). This is what we exepcted - that's what happens in the normal affix tagger."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Entropy Voting"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The requested classes are defined in voting.py, below are the answers for the questions in the end of the section:\n",
      "1.\n",
      "The entropy voting tagger scored:\n",
      "    0.934806038849\n",
      "while running Unigram with Affix backoff scored:\n",
      "    0.938397435393"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "we can see the entropy backoff strategy is not necessarily better. we imagine the heap of statistics we're about to generate will help us understand why.\n",
      "2.1. UnigramTagger (with no backoff) has no information (unknown word)\n",
      "tokens: 6921 (5.6%)\n",
      "unique words: 4757 (29%)\n",
      "\n",
      "2.2. UnigramTagger has exactly i tag options (for i in [1...12]).\n",
      "tokens: \n",
      "{1: 56131, 2: 37627, 3: 13254, 4: 8261, 5: 1144, 6: 69}\n",
      "{1: 0.45, 2: 0.3, 3: 0.11, 4: 0.07, 5: 0.01, 6: 0.0}\n",
      "\n",
      "unique words: \n",
      "{1: 9260, 2: 1750, 3: 228, 4: 43, 5: 6, 6: 1}\n",
      "{1: 0.58, 2: 0.11, 3: 0.01, 4: 0.0, 5: 0.0, 6: 0.0}\n",
      "\n",
      "2.3. UnigramTagger has an entropy over 0.69 (which is the entropy of a 50%/50% bet).\n",
      "(since we used log2, we counted entropy over 1)\n",
      "tokens: 2292 (1.85%)\n",
      "words: 93 (0.057%)\n",
      "\n",
      "3. Draw a plot of the error rate of the unigram tagger as a function of the entropy.\n",
      "Assuming we consider None an error:\n",
      "error rates (None=error) =  [(0.0, 0.5770996936018715), (0.1, 0.27396246032183613), (0.2, 0.22664797296765093), (0.3, 0.21289550169739835), (0.4, 0.19455099677848753), (0.5, 0.17860884222465523), (0.6, 0.16773131483392278), (0.7, 0.1534274844634882), (0.8, 0.1427941303885506), (0.9, 0.13102655187895307), (1.0, 0.10392331382572328), (1.1, 0.1004024921431328), (1.2, 0.09960695972715605), (1.3, 0.09878779763545709), (1.4, 0.09232114304618022)]\n",
      "\n",
      "4. First plot is where None concidered as errors.\n",
      "As we can see, when entropy grows ,the error error dicreasing.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"graphs/error_rate_unigram_nones_are_error.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this graph Nones are not concidered as errors:\n",
      "<img src=\"graphs/error_rate_unigram_nones_are_correct.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see, when entropy grows , we have more mistakes (not None mistakes), that we cannot fix in backoff tagger."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at the disagreements, they were almost all when the Affix tagger had lower entropy but the wrong tag. we learn that the Unigram tagger is almost always correct even when it has higher entropy than the Affix tagger.\n",
      "We dumped disagreement output to disagreement_voting_backoff.txt in /src folder"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.part2 import *\n",
      "all_words = corpus.brown.tagged_sents(tagset=\"universal\")\n",
      "print get_all_tags(all_words)\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "\n",
      "from nltk import UnigramTagger\n",
      "\n",
      "u_tagger = UnigramTagger(train)\n",
      "get_statistics_per_tag(dev, u_tagger)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set([u'ADV', u'NOUN', u'ADP', u'PRON', u'DET', u'.', u'PRT', u'VERB', u'X', u'NUM', u'CONJ', u'ADJ'])\n",
        "results for tag u'ADV' are: TP: 3820 TN: 118460 FP: 618 FN: 503 precision: 0.8607480847228481 recall 0.8836456164700439 f_measure 0.8720465700262527"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'NOUN' are: TP: 29528 TN: 86052 FP: 6790 FN: 1031 precision: 0.8130403656589019 recall 0.9662619850125986 f_measure 0.8830539647412413"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'ADP' are: TP: 13832 TN: 107486 FP: 1306 FN: 777 precision: 0.9137270445237151 recall 0.9468136080498323 f_measure 0.9299761320469291"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'PRON' are: TP: 3225 TN: 119970 FP: 202 FN: 4 precision: 0.9410563174788444 recall 0.9987612263858779 f_measure 0.9690504807692307"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'DET' are: TP: 13945 TN: 109214 FP: 183 FN: 59 precision: 0.9870469988674971 recall 0.9957869180234219 f_measure 0.9913976965732973"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'.' are: TP: 14377 TN: 109023 FP: 0 FN: 1 precision: 1.0 recall 0.9999304492975379 f_measure 0.9999652234394019"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'PRT' are: TP: 2700 TN: 119236 FP: 177 FN: 1288 precision: 0.9384775808133472 recall 0.6770310932798396 f_measure 0.7865986890021851"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'VERB' are: TP: 16802 TN: 104246 FP: 1427 FN: 926 precision: 0.9217181414230073 recall 0.9477662454873647 f_measure 0.9345607253107878"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'X' are: TP: 44 TN: 123197 FP: 79 FN: 81 precision: 0.35772357723577236 recall 0.352 f_measure 0.3548387096774193"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'NUM' are: TP: 2115 TN: 120999 FP: 271 FN: 16 precision: 0.8864207879295892 recall 0.992491787893008 f_measure 0.9364622537082133"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'CONJ' are: TP: 3432 TN: 119933 FP: 3 FN: 33 precision: 0.9991266375545852 recall 0.9904761904761905 f_measure 0.9947826086956523"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "results for tag u'ADJ' are: TP: 7129 TN: 114064 FP: 1396 FN: 812 precision: 0.8362463343108505 recall 0.8977458758342778 f_measure 0.8659055022470545"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "best tag:  .\n",
        "worst tag:  X\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We tested statistics of tags in full brown corpus with universal tagset , and we got following results:\n",
      "set([u'ADV', u'NOUN', u'ADP', u'PRON', u'DET', u'.', u'PRT', u'VERB', u'X', u'NUM', u'CONJ', u'ADJ']) - all possible tags\n",
      "results for tag u'ADV' are: TP: 3820 TN: 118460 FP: 618 FN: 503 precision: 0.8607480847228481 recall 0.8836456164700439 f_measure 0.8720465700262527\n",
      "results for tag u'NOUN' are: TP: 29528 TN: 86052 FP: 6790 FN: 1031 precision: 0.8130403656589019 recall 0.9662619850125986 f_measure 0.8830539647412413\n",
      "results for tag u'ADP' are: TP: 13832 TN: 107486 FP: 1306 FN: 777 precision: 0.9137270445237151 recall 0.9468136080498323 f_measure 0.9299761320469291\n",
      "results for tag u'PRON' are: TP: 3225 TN: 119970 FP: 202 FN: 4 precision: 0.9410563174788444 recall 0.9987612263858779 f_measure 0.9690504807692307\n",
      "results for tag u'DET' are: TP: 13945 TN: 109214 FP: 183 FN: 59 precision: 0.9870469988674971 recall 0.9957869180234219 f_measure 0.9913976965732973\n",
      "results for tag u'.' are: TP: 14377 TN: 109023 FP: 0 FN: 1 precision: 1.0 recall 0.9999304492975379 f_measure 0.9999652234394019\n",
      "results for tag u'PRT' are: TP: 2700 TN: 119236 FP: 177 FN: 1288 precision: 0.9384775808133472 recall 0.6770310932798396 f_measure 0.7865986890021851\n",
      "results for tag u'VERB' are: TP: 16802 TN: 104246 FP: 1427 FN: 926 precision: 0.9217181414230073 recall 0.9477662454873647 f_measure 0.9345607253107878\n",
      "results for tag u'X' are: TP: 44 TN: 123197 FP: 79 FN: 81 precision: 0.35772357723577236 recall 0.352 f_measure 0.3548387096774193\n",
      "results for tag u'NUM' are: TP: 2115 TN: 120999 FP: 271 FN: 16 precision: 0.8864207879295892 recall 0.992491787893008 f_measure 0.9364622537082133\n",
      "results for tag u'CONJ' are: TP: 3432 TN: 119933 FP: 3 FN: 33 precision: 0.9991266375545852 recall 0.9904761904761905 f_measure 0.9947826086956523\n",
      "results for tag u'ADJ' are: TP: 7129 TN: 114064 FP: 1396 FN: 812 precision: 0.8362463343108505 recall 0.8977458758342778 f_measure 0.8659055022470545\n",
      "\n",
      "As we can see, The most problematic tag is X(other)\tersatz, esprit, dunno, gr8, univeristy. And it makes sense.\n",
      "Second most problematic tag is 'PTR'.\n",
      "\n",
      "We decided to remove from concideration tags that were  present only few times.\n",
      "best tag:  BEDZ*\n",
      "worst tag:  BEZ-HL\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Confusion Matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Validating over defualt tagger:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.confusion import *\n",
      "\n",
      "all_words = nltk.corpus.brown.tagged_sents(tagset='universal', categories='news')\n",
      "ds_length = len(all_words)\n",
      "train = all_words[int(0.2 * ds_length):]\n",
      "dev = all_words[:int(0.1 * ds_length)]\n",
      "test = all_words[int(0.1 * ds_length):int(0.2 * ds_length)]\n",
      "\n",
      "print confusion_matrix(dev, DefaultTagger('NOUN'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     |                        C         N         P         V      |\n",
        "     |         A    A    A    O    D    O    N    R    P    E      |\n",
        "     |         D    D    D    N    E    U    U    O    R    R      |\n",
        "     |    .    J    P    V    J    T    N    M    N    T    B    X |\n",
        "-----+-------------------------------------------------------------+\n",
        "   . |   <.>   .    .    .    .    . 1142    .    .    .    .    . |\n",
        " ADJ |    .   <.>   .    .    .    .  731    .    .    .    .    . |\n",
        " ADP |    .    .   <.>   .    .    . 1383    .    .    .    .    . |\n",
        " ADV |    .    .    .   <.>   .    .  311    .    .    .    .    . |\n",
        "CONJ |    .    .    .    .   <.>   .  237    .    .    .    .    . |\n",
        " DET |    .    .    .    .    .   <.>1259    .    .    .    .    . |\n",
        "NOUN |    .    .    .    .    .    .<3298>   .    .    .    .    . |\n",
        " NUM |    .    .    .    .    .    .  205   <.>   .    .    .    . |\n",
        "PRON |    .    .    .    .    .    .  245    .   <.>   .    .    . |\n",
        " PRT |    .    .    .    .    .    .  251    .    .   <.>   .    . |\n",
        "VERB |    .    .    .    .    .    . 1718    .    .    .   <.>   . |\n",
        "   X |    .    .    .    .    .    .    5    .    .    .    .   <.>|\n",
        "-----+-------------------------------------------------------------+\n",
        "(row = reference; col = test)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Results over the entire corpus for the best backoff tagger:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "success rate for class tagger =  0.942407273847\n",
      "     |                             C           N           P           V       |\n",
      "     |           A     A     A     O     D     O     N     R     P     E       |\n",
      "     |           D     D     D     N     E     U     U     O     R     R       |\n",
      "     |     .     J     P     V     J     T     N     M     N     T     B     X |\n",
      "-----+-------------------------------------------------------------------------+\n",
      "   . |<14377>    .     .     .     .     .     .     .     .     .     .     . |\n",
      " ADJ |     . <7350>   23   429     .    12   462     7     .     1   182    59 |\n",
      " ADP |     .     7<13832>   54    13     3     6     .     .  1219     4     . |\n",
      " ADV |     .   217   252 <3863>    9    11    20     .     .    61     5     . |\n",
      "CONJ |     .     .     .     3 <3432>    .     .     .     .     .     .     . |\n",
      " DET |     .     .   170     .    11<13945>    .     .     1     1     .     . |\n",
      "NOUN |     1   747    53    90     .    40<34183>   79     3    11  1069    42 |\n",
      " NUM |     .     .     2     .     .     .   216 <2164>    .     .     1     3 |\n",
      "PRON |     .     .   169     .     .    33     .     . <3225>    .     .     . |\n",
      " PRT |     .    25   135     3     .     .     9     .     . <2700>    5     . |\n",
      "VERB |     .    67    28    13     .     .   942     .     .     .<17179>    . |\n",
      "   X |     .    12     .     .     .     2    62     .     .     2     1   <44>|\n",
      "-----+-------------------------------------------------------------------------+\n",
      "(row = reference; col = test)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most common confusion is adposition reported as particle. Noun was also reported as adjective and verb a sizable amount of times.\n",
      "\n",
      "Using a 2gram tagger before the unigram tagger somewhat improves this - adpositions usually appear before particles, so it makes sense looking at two words at a time avoids this misreport.\n",
      "results:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "     |                             C           N           P           V       |\n",
      "     |           A     A     A     O     D     O     N     R     P     E       |\n",
      "     |           D     D     D     N     E     U     U     O     R     R       |\n",
      "     |     .     J     P     V     J     T     N     M     N     T     B     X |\n",
      "-----+-------------------------------------------------------------------------+\n",
      "   . |<14377>    .     .     .     .     .     .     .     .     .     .     . |\n",
      " ADJ |     . <7423>   29   303     .    12   535     7     .     2   175    39 |\n",
      " ADP |     .     7<13739>   72    11    18     8     .   198  1067     3    15 |\n",
      " ADV |     .   155   221 <3958>   12     8    19     .     1    54     5     5 |\n",
      "CONJ |     .     .     .     1 <3434>    .     .     .     .     .     .     . |\n",
      " DET |     .     .    79    13    11<13963>    9     .    46     .     .     7 |\n",
      "NOUN |     1   666    45    81     .    33<34519>   79     1    17   811    65 |\n",
      " NUM |     .     .     2     .     .     .   222 <2159>    .     .     1     2 |\n",
      "PRON |     .     .    11     .     .    17     .     . <3398>    .     .     1 |\n",
      " PRT |     .    25   202    49     .     .     9     .     . <2583>    5     4 |\n",
      "VERB |     .    44    25    13     .     .   575     .     .     2<17564>    6 |\n",
      "   X |     .     5     .     .     .     2    55     .     .     2     1   <58>|\n",
      "-----+-------------------------------------------------------------------------+\n",
      "(row = reference; col = test)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Hebrew stuff"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing Norvig's Word Segmenter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.testing_norvig import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}