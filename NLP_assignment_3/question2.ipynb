{
 "metadata": {
  "signature": "sha256:2a717a17389cc3589f1f7c616de422cd99a8395645fe2bd78643e56b6e678809"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Question 2.1: Build a Parser"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we induce a PCFG and build a parser, we note that we must handle unknown words when we parse sentences. <br/>\n",
      "We do this by using POS tagging; before parsing a sentence, we replace each unknown word with its Part of Speech tag. As a result, the parser will have to know the Production (Penn-POS -> Universal-POS) where the Universal-POS is a terminal node replacing the original word in the sentence.<br/>\n",
      "To acheive this we define two methods:<br/>\n",
      "<ol>\n",
      "<li><b>get_parser -</b> gets a list of parsed trees, and returns (parser, pcfg) that include the POS rules mentioned above.</li>\n",
      "<li><b>pos_uncovered_tokens -</b> gets a sentence and a pcfg, and replaces each word not known in the pcfg by its part of speech (accoridng to the best tagger learned in Assignment 1)</li> \n",
      "</ol>\n",
      "As well as a mapping from Penn-POS tags to Universal-POS tags, <b>GRAMMAR_TO_POS</b>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import induce_pcfg, Nonterminal, ViterbiParser, Tree, Production, DefaultTagger\n",
      "from nltk.corpus import LazyCorpusLoader, BracketParseCorpusReader\n",
      "from question1 import filter_tree, tree_to_productions, pcfg_cnf_learn\n",
      "\n",
      "from ass1 import *\n",
      "from nltk import UnigramTagger\n",
      "\n",
      "GRAMMAR_TO_POS = {\n",
      "    \"CC\": \"CONJ\",\n",
      "    \"CD\": \"NUM\",\n",
      "    \"DT\": \"DET\",\n",
      "    \"EX\": \"ADVERB\",\n",
      "    \"FW\": \"X\",\n",
      "    \"IN\": \"ADP\",\n",
      "    \"JJ\": \"ADJ\",\n",
      "    \"JJR\": \"ADJ\",\n",
      "    \"JJS\": \"ADJ\",\n",
      "    \"LS\": \".\",\n",
      "    \"MD\": \"VERB\",\n",
      "    \"NN\": \"NOUN\",\n",
      "    \"NNS\": \"NOUN\",\n",
      "    \"NNP\": \"NOUN\",\n",
      "    \"NNPS\": \"NOUN\",\n",
      "    \"PDT\": \"DET\",\n",
      "    \"POS\": \"PRON\",\n",
      "    \"PRP\": \"PRON\",\n",
      "    \"PRP$\": \"PRON\",\n",
      "    \"RB\": \"ADV\",\n",
      "    \"RBR\": \"ADV\",\n",
      "    \"RBS\": \"ADV\",\n",
      "    \"RP\": \"PRT\",\n",
      "    \"SYM\": \".\",\n",
      "    \"TO\": \"ADP\",\n",
      "    \"UH\": \"PRT\",\n",
      "    \"VB\": \"VERB\",\n",
      "    \"VBD\": \"VERB\",\n",
      "    \"VBG\": \"VERB\",\n",
      "    \"VBN\": \"VERB\",\n",
      "    \"VBP\": \"VERB\",\n",
      "    \"WDT\": \"DET\",\n",
      "    \"WP\": \"PRON\",\n",
      "    \"WP$\": \"PRON\",\n",
      "    \"WRB\": \"ADV\",\n",
      "}\n",
      "\n",
      "def get_pos_tagger():\n",
      "    all_words = corpus.brown.tagged_sents(tagset='universal')\n",
      "    train = all_words\n",
      "\n",
      "    u0 = UnigramTagger(train, backoff=DefaultTagger(\"NOUN\"))\n",
      "\n",
      "    return EntropyAffixTagger(train=train, cutoff=0.5, backoff=u0)\n",
      "\n",
      "\n",
      "tagger = get_pos_tagger()\n",
      "\n",
      "\n",
      "def pos_uncovered_tokens(test_sentence, training_pcfg):\n",
      "    pos = tagger.tag(test_sentence)\n",
      "    for i, token in enumerate(test_sentence):\n",
      "        if token not in training_pcfg._lexical_index:\n",
      "            test_sentence[i] = \"$\" + pos[i][1]\n",
      "\n",
      "    return test_sentence\n",
      "\n",
      "\n",
      "def get_parser(training_trees):\n",
      "    training_prods = sum([list(tree_to_productions(t)) for t in training_trees], list())\n",
      "    pos_rules = [Production(Nonterminal(lhs), [\"$\" + rhs]) for lhs, rhs in GRAMMAR_TO_POS.iteritems()]\n",
      "    training_prods += pos_rules\n",
      "    training_pcfg = induce_pcfg(Nonterminal(\"S\"), training_prods)\n",
      "    parser = ViterbiParser(training_pcfg)\n",
      "\n",
      "    return parser, training_pcfg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we test the annotating function on the first sentence in the test set (which already contains unknown words)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg')\n",
      "trees = treebank.parsed_sents()\n",
      "eighty_perc = int(len(trees) * 0.8)\n",
      "training_trees = pcfg_cnf_learn(treebank, eighty_perc)\n",
      "test_trees = trees[eighty_perc:]\n",
      "parser, training_pcfg = get_parser(training_trees)\n",
      "\n",
      "test_tree = filter_tree(test_trees[0])\n",
      "test_sentence = test_tree.leaves()\n",
      "test_sentence = pos_uncovered_tokens(test_sentence, training_pcfg)\n",
      "\n",
      "print test_sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'The', u'latest', u'10-year', u'notes', u'were', u'quoted', u'at', u'100', u'22\\\\/32', u'to', u'yield', u'7.88', u'%', u'compared', u'with', u'100', '$NOUN', u'to', u'yield', u'7.90', u'%', u'.']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see it replaced unknown tokens by a part of speech, now let us try the parser:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print list(parser.parse(test_sentence))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ProbabilisticTree('S', [ProbabilisticTree('NP^<S>', [ProbabilisticTree('DT', ['The']) (p=0.08558352402745996), ProbabilisticTree('NP|<JJS>^<S>', [ProbabilisticTree('JJS', ['latest']) (p=0.0896551724137931), ProbabilisticTree('NP|<CD>^<S>', [ProbabilisticTree('CD', ['10-year']) (p=0.00039761431411530816), ProbabilisticTree('NNS', ['notes']) (p=0.005080268238162975)]) (p=6.733291236796521e-07)]) (p=6.707493186080824e-09)]) (p=8.485600949814815e-13), ProbabilisticTree('S|<VP>', [ProbabilisticTree('VP^<S>', [ProbabilisticTree('VBD', ['were']) (p=0.06388526727509779), ProbabilisticTree('VP^<VP>', [ProbabilisticTree('VBN', ['quoted']) (p=0.0029708853238265003), ProbabilisticTree('VP|<PP>^<VP>', [ProbabilisticTree('PP^<VP>', [ProbabilisticTree('IN', ['at']) (p=0.04136099165190994), ProbabilisticTree('NP^<PP>', [ProbabilisticTree('QP^<NP>', [ProbabilisticTree('CD', ['100']) (p=0.012723658051689861), ProbabilisticTree('CD', ['22\\\\/32']) (p=0.00039761431411530816)]) (p=6.247077383094805e-07)]) (p=9.239646330539416e-09)]) (p=2.802338839278382e-10), ProbabilisticTree('S^<VP>', [ProbabilisticTree('VP^<S>', [ProbabilisticTree('TO', ['to']) (p=0.9889277389277389), ProbabilisticTree('VP^<VP>', [ProbabilisticTree('VB', ['yield']) (p=0.010209042294603793), ProbabilisticTree('VP|<NP>^<VP>', [ProbabilisticTree('NP^<VP>', [ProbabilisticTree('NP^<NP>', [ProbabilisticTree('CD', ['7.88']) (p=0.0011928429423459245), ProbabilisticTree('NN', ['%']) (p=0.031950126631599456)]) (p=5.223329571072206e-07), ProbabilisticTree('VP^<NP>', [ProbabilisticTree('VBN', ['compared']) (p=0.0106951871657754), ProbabilisticTree('PP^<VP>', [ProbabilisticTree('IN', ['with']) (p=0.03933721224386542), ProbabilisticTree('NP^<PP>', [ProbabilisticTree('CD', ['100']) (p=0.012723658051689861), ProbabilisticTree('NNS', ['$NOUN']) (p=0.000203210729526519)]) (p=1.670753483225788e-08)]) (p=4.81937044161947e-10)]) (p=1.9845844707975737e-12)]) (p=1.7281469738403862e-20), ProbabilisticTree('S^<VP>', [ProbabilisticTree('VP^<S>', [ProbabilisticTree('TO', ['to']) (p=0.9889277389277389), ProbabilisticTree('VP^<VP>', [ProbabilisticTree('VB', ['yield']) (p=0.010209042294603793), ProbabilisticTree('NP^<VP>', [ProbabilisticTree('CD', ['7.90']) (p=0.0011928429423459245), ProbabilisticTree('NN', ['%']) (p=0.031950126631599456)]) (p=5.143386176336758e-07)]) (p=8.888250145210049e-10)]) (p=1.2109687251364533e-10)]) (p=8.788173033847405e-11)]) (p=1.033816360825318e-31)]) (p=9.784692403897867e-35)]) (p=1.3331033997266647e-35)]) (p=9.674521815159225e-36)]) (p=2.223638782604839e-46)]) (p=1.7891726191982884e-50)]) (p=4.134050800170843e-53), ProbabilisticTree('.', ['.']) (p=0.9858611825192802)]) (p=3.780448885503274e-53)]) (p=1.599421990770694e-65)]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Question 2.2 - Metrics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from question2 import *\n",
      "\n",
      "ACCURACY_PER_DISTANCE = {}\n",
      "ACCURACY_PER_DISTANCE_LABELED = {}\n",
      "ACCURACY_PER_LABEL = {}\n",
      "\n",
      "\n",
      "def exist_same(con, cons_list):\n",
      "    for item in cons_list:\n",
      "        if (item[0].label().split('^')[0].split('|')[0] == con[0].label().split('^')[0].split('|')[0]) and (item[1] == con[1]) and (item[2] == con[2]):\n",
      "            return True\n",
      "        if (item[0].label() == con[0].label()) and (item[1] == con[1]) and (item[2] == con[2]):\n",
      "            return True\n",
      "\n",
      "    return False\n",
      "\n",
      "\n",
      "def calculate_index_metrics(origin_cons, guess_cons):\n",
      "    origin_indexes = set([(x[1], x[2]) for x in origin_cons])\n",
      "    guess_indexes = set([(x[1], x[2]) for x in guess_cons])\n",
      "    origin_len = len(origin_indexes)\n",
      "    guess_len = len(guess_indexes)\n",
      "    pre_count = 0\n",
      "    recall_count = 0\n",
      "    for item in guess_indexes:\n",
      "        distance = item[1]-item[0]+1\n",
      "        if distance not in ACCURACY_PER_DISTANCE:\n",
      "            ACCURACY_PER_DISTANCE[distance] = {'total': 0, 'matches': 0}\n",
      "\n",
      "        ACCURACY_PER_DISTANCE[distance]['total'] += 1\n",
      "\n",
      "        if item in origin_indexes:\n",
      "            ACCURACY_PER_DISTANCE[distance]['matches'] += 1\n",
      "            pre_count += 1\n",
      "\n",
      "    for item in origin_indexes:\n",
      "        if item in guess_indexes:\n",
      "            recall_count += 1\n",
      "\n",
      "    recall = float(recall_count)/float(origin_len)\n",
      "    precision = float(pre_count)/float(guess_len)\n",
      "    f_measure = 2*(recall*precision)/(recall + precision)\n",
      "\n",
      "    return precision, recall, f_measure\n",
      "\n",
      "\n",
      "def calculate_joint_metrics(origin_cons, guess_cons):\n",
      "    origin_cons = list(origin_cons)\n",
      "    guess_cons = list(guess_cons)\n",
      "    origin_len = len(list(origin_cons))\n",
      "    guess_len = len(list(guess_cons))\n",
      "\n",
      "    pre_count = 0\n",
      "    recall_count = 0\n",
      "    # calculate precision\n",
      "    for item in guess_cons:\n",
      "        distance = item[2]-item[1]+1\n",
      "        label = item[0].label()\n",
      "        if label not in ACCURACY_PER_LABEL:\n",
      "            ACCURACY_PER_LABEL[label] = {'total': 0, 'matches': 0}\n",
      "        if distance not in ACCURACY_PER_DISTANCE_LABELED:\n",
      "            ACCURACY_PER_DISTANCE_LABELED[distance] = {'total': 0, 'matches': 0}\n",
      "        ACCURACY_PER_LABEL[label]['total'] += 1\n",
      "        ACCURACY_PER_DISTANCE_LABELED[distance]['total'] += 1\n",
      "        if exist_same(item, origin_cons):\n",
      "            ACCURACY_PER_DISTANCE_LABELED[distance]['matches'] += 1\n",
      "            ACCURACY_PER_LABEL[label]['matches'] += 1\n",
      "            pre_count += 1\n",
      "\n",
      "    for item in origin_cons:\n",
      "        if exist_same(item, guess_cons):\n",
      "            recall_count += 1\n",
      "\n",
      "    recall = float(recall_count)/float(origin_len)\n",
      "    precision = float(pre_count)/float(guess_len)\n",
      "    f_measure = 2*(recall*precision)/(recall + precision)\n",
      "\n",
      "    return precision, recall, f_measure\n",
      "\n",
      "def calculate_accuracy_per_distance():\n",
      "    x_axis = ACCURACY_PER_DISTANCE.keys()\n",
      "    x_axis.sort()\n",
      "    y_axis = [ACCURACY_PER_DISTANCE[x]['matches']/float(ACCURACY_PER_DISTANCE[x]['total']) for x in x_axis]\n",
      "    x_axis_labeled = ACCURACY_PER_DISTANCE_LABELED.keys()\n",
      "    x_axis_labeled.sort()\n",
      "    y_axis_labeled = [ACCURACY_PER_DISTANCE_LABELED[x]['matches']/float(ACCURACY_PER_DISTANCE_LABELED[x]['total']) for x in x_axis_labeled]\n",
      "    print x_axis\n",
      "    print y_axis\n",
      "    import matplotlib.pyplot as plt\n",
      "    plt.title(\"Accuracy per distance\")\n",
      "    plt.scatter(x_axis, y_axis, c=\"blue\", marker='*', label=\"accuracy index\")\n",
      "    plt.scatter(x_axis_labeled, y_axis_labeled, c=\"red\", marker='o', label=\"accuracy label\", alpha=0.5)\n",
      "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "def eval_tree(orig_tree, guess_tree):\n",
      "    origin_cons = tree_to_constituents(orig_tree)\n",
      "    guess_cons = tree_to_constituents(guess_tree)\n",
      "    origin_cons = list(origin_cons)\n",
      "    guess_cons = list(guess_cons)\n",
      "    a, b, c = calculate_joint_metrics(origin_cons, guess_cons)\n",
      "    d, e, f = calculate_index_metrics(origin_cons,guess_cons)\n",
      "    return (a,b), (d,e)\n",
      "\n",
      "\n",
      "def eval_trees(trees, parser, pcfg):\n",
      "    counter = 0\n",
      "    overall_prec_labeled = 0\n",
      "    overall_recall_labeled = 0\n",
      "    overall_prec_index = 0\n",
      "    overall_recall_index = 0\n",
      "    for tree in trees:\n",
      "        counter += 1\n",
      "        tokens = pos_uncovered_tokens(tree.leaves(), pcfg)\n",
      "        guess_tree = parser.parse(tokens)\n",
      "        guess_tree = list(guess_tree)[0]\n",
      "        # guess_tree.draw()\n",
      "        # tree.draw()\n",
      "        labaled_metrics, index_metrics = eval_tree(tree, guess_tree)\n",
      "        pre_labeled, recall_labeled = labaled_metrics\n",
      "        pre_index, recall_index = index_metrics\n",
      "        overall_prec_labeled += pre_labeled\n",
      "        overall_recall_labeled += recall_labeled\n",
      "        overall_prec_index += pre_index\n",
      "        overall_recall_index += recall_index\n",
      "\n",
      "    overall_recall_labeled = overall_recall_labeled/float(counter)\n",
      "    overall_prec_labeled = overall_prec_labeled/float(counter)\n",
      "    overall_recall_index = overall_recall_index/float(counter)\n",
      "    overall_prec_index = overall_prec_index/float(counter)\n",
      "\n",
      "    print \"precision for labeled: \", overall_prec_labeled\n",
      "    print \"recall_labeled: \", overall_recall_labeled\n",
      "    print \"fmeasure labeled \", 2*(overall_prec_labeled*overall_recall_labeled)/(overall_prec_labeled+overall_recall_labeled)\n",
      "    print\n",
      "    print \"precision for index: \", overall_prec_index\n",
      "    print \"recall_index: \", overall_recall_index\n",
      "    print \"fmeasure index \", 2*(overall_prec_index*overall_recall_index)/(overall_prec_index+overall_recall_index)\n",
      "    \n",
      "\n",
      "cleaned_trees = [filter_tree(t) for t in test_trees[:1]]\n",
      "for t in cleaned_trees:\n",
      "    chomsky_normal_form(t, factor='right', horzMarkov=1, vertMarkov=1, childChar='|', parentChar='^')\n",
      "    \n",
      "eval_trees(cleaned_trees, parser, training_pcfg)\n",
      "\n",
      "print \"----------- Reporting Per Label -----------\"\n",
      "print ACCURACY_PER_LABEL\n",
      "for item in ACCURACY_PER_LABEL:\n",
      "    print item, \"--- total -------> \", ACCURACY_PER_LABEL[item]['total']\n",
      "    print item, \"--- precision ---> \", ACCURACY_PER_LABEL[item]['matches']/float(ACCURACY_PER_LABEL[item]['total'])\n",
      "print '-'*100\n",
      "\n",
      "print ACCURACY_PER_DISTANCE\n",
      "calculate_accuracy_per_distance()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "with filtered annotations:\n",
      "    \n",
      "precision for labeled:  0.700300326027\n",
      "recall_labeled:  0.712663530466\n",
      "fmeasure labeled  0.70642784023\n",
      "precision for index:  0.929015669352\n",
      "recall_index:  0.929015669352\n",
      "fmeasure index  0.929015669352"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}